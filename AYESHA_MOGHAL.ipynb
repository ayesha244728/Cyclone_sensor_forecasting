{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21341,"status":"ok","timestamp":1759361882174,"user":{"displayName":"Ayesha Moghal","userId":"13341639430554687722"},"user_tz":-330},"id":"ppOC0hbrIyNB","outputId":"e4d78b99-d70f-4297-913d-d0e74ac2bf59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135429,"status":"ok","timestamp":1759570034979,"user":{"displayName":"Ayesha Moghal","userId":"13341639430554687722"},"user_tz":-330},"id":"qy_jcuarLyDT","outputId":"e119a73e-82e1-4fc8-f71a-aed85d337b61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Process completed. CSV files and plots are saved in the current directory.\n"]}],"source":["# Task 1\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","from sklearn.ensemble import IsolationForest\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import mean_squared_error\n","from statsmodels.tsa.arima.model import ARIMA\n","import os\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","os.makedirs(\"plots\", exist_ok=True)\n","pd.set_option(\"display.max_columns\", None)\n","plt.style.use(\"seaborn-v0_8\")\n","sns.set_palette(\"pastel\")\n","\n","def load_data():\n","    df = pd.read_excel(\"data.xlsx\")\n","    time_col = \"time\"\n","    if time_col not in df.columns:\n","        raise RuntimeError(\"Missing time column\")\n","    feature_cols = [col for col in df.columns if col != time_col]\n","    invalid_vals = ['I/O Timeout', 'Not Connect', 'Error', 'No Data', 'Disconnected', '', ' ']\n","    df[feature_cols] = df[feature_cols].replace(invalid_vals, np.nan).apply(pd.to_numeric, errors='coerce')\n","    df[time_col] = pd.to_datetime(df[time_col])\n","    df = df.sort_values(time_col).reset_index(drop=True)\n","    df.set_index(time_col, inplace=True)\n","    return df, feature_cols\n","\n","def exploratory_analysis(df, features):\n","    corr_mat = df[features].corr()\n","    plt.figure(figsize=(11,9))\n","    mask = np.triu(np.ones_like(corr_mat, dtype=bool))\n","    sns.heatmap(corr_mat, mask=mask, annot=True, cmap=\"vlag\", center=0, fmt=\".2f\")\n","    plt.title(\"Feature Correlation Matrix\")\n","    plt.tight_layout()\n","    plt.savefig(\"plots/correlation_heatmap.png\", dpi=300)\n","    plt.close()\n","\n","    fig, axs = plt.subplots(3, 2, figsize=(14, 12))\n","    axs = axs.flatten()\n","    for i, feature in enumerate(features[:6]):\n","        axs[i].plot(df.index[:2016], df[feature].values[:2016], lw=0.8)\n","        axs[i].set_title(f\"{feature} — 2-Week View\")\n","        axs[i].tick_params(axis='x', rotation=45)\n","    plt.tight_layout()\n","    plt.savefig(\"plots/sample_trends.png\", dpi=300)\n","    plt.close()\n","\n","def identify_shutdowns(df, features):\n","    q_low = dict()\n","    q_low['Cyclone_Inlet_Gas_Temp'] = df['Cyclone_Inlet_Gas_Temp'].quantile(0.1)\n","    q_low['Cyclone_Gas_Outlet_Temp'] = df['Cyclone_Gas_Outlet_Temp'].quantile(0.1)\n","    q_low['Cyclone_Material_Temp'] = df['Cyclone_Material_Temp'].quantile(0.15)\n","    shutdown_flag = (\n","        (df['Cyclone_Inlet_Gas_Temp'] < q_low['Cyclone_Inlet_Gas_Temp']) &\n","        (df['Cyclone_Gas_Outlet_Temp'] < q_low['Cyclone_Gas_Outlet_Temp']) &\n","        (df['Cyclone_Material_Temp'] < q_low['Cyclone_Material_Temp'])\n","    )\n","\n","    intervals = []\n","    ongoing = False\n","    start_time = None\n","    for ts, val in shutdown_flag.items():\n","        if val and not ongoing:\n","            ongoing = True\n","            start_time = ts\n","        elif not val and ongoing:\n","            ongoing = False\n","            duration = (ts - start_time).total_seconds() / 3600\n","            intervals.append({\"start\": start_time, \"end\": ts, \"duration_hrs\": duration})\n","    if ongoing and start_time is not None:\n","        duration = (df.index[-1] - start_time).total_seconds() / 3600\n","        intervals.append({\"start\": start_time, \"end\": df.index[-1], \"duration_hrs\": duration})\n","\n","    shutdown_df = pd.DataFrame(intervals)\n","    plt.figure(figsize=(16, 7))\n","    plt.plot(df.index[:4032], df[\"Cyclone_Inlet_Gas_Temp\"][:4032], label=\"Cyclone Inlet Gas Temp\")\n","    for _, row in shutdown_df.iterrows():\n","        plt.axvspan(row[\"start\"], row[\"end\"], color=\"red\", alpha=0.3)\n","    plt.title(\"Detected Shutdown Periods (2 Weeks)\")\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Temperature\")\n","    plt.legend()\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.savefig(\"plots/shutdown_periods.png\", dpi=300)\n","    plt.close()\n","\n","    if not shutdown_df.empty:\n","        shutdown_df.to_csv(\"shutdown_periods.csv\", index=False)\n","\n","    return shutdown_flag, shutdown_df\n","\n","def cluster_states(df, features, shutdown_flag):\n","    active_df = df.loc[~shutdown_flag].copy()\n","    window = 12\n","\n","    for feat in features:\n","        active_df[f\"{feat}_mean\"] = active_df[feat].rolling(window=window, min_periods=1).mean()\n","        active_df[f\"{feat}_std\"] = active_df[feat].rolling(window=window, min_periods=1).std()\n","        active_df[f\"{feat}_diff\"] = active_df[feat].diff()\n","        active_df[f\"{feat}_lag1\"] = active_df[feat].shift(1)\n","\n","    cluster_features = []\n","    for feat in features:\n","        cluster_features.extend([feat, f\"{feat}_mean\", f\"{feat}_std\", f\"{feat}_diff\", f\"{feat}_lag1\"])\n","    cluster_features = [f for f in cluster_features if f in active_df.columns]\n","\n","    cluster_data = active_df[cluster_features].dropna()\n","\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(cluster_data)\n","\n","    kmeans = KMeans(n_clusters=4, random_state=1, n_init=15)\n","    labels = kmeans.fit_predict(X)\n","\n","    cluster_data[\"cluster\"] = labels\n","    summary = cluster_data.groupby(\"cluster\")[features].mean().reset_index()\n","    cluster_counts = cluster_data[\"cluster\"].value_counts().sort_index()\n","    summary['count'] = cluster_counts.values\n","    summary['percentage'] = 100 * summary['count'] / len(cluster_data)\n","    summary.to_csv(\"cluster_summary.csv\", index=False)\n","\n","    pca = PCA(n_components=2)\n","    X_pca = pca.fit_transform(X)\n","    plt.figure(figsize=(11, 8))\n","    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', alpha=0.6)\n","    plt.title(\"Clusters (PCA Reduced)\")\n","    plt.tight_layout()\n","    plt.savefig(\"plots/clusters_pca.png\", dpi=300)\n","    plt.close()\n","\n","    return cluster_data, labels, summary\n","\n","def detect_anomalies(clustered_df, labels, features):\n","    anomaly_records = []\n","    for cluster_id in np.unique(labels):\n","        subset = clustered_df[clustered_df[\"cluster\"] == cluster_id]\n","        relevant_features = features + [f\"{feat}_std\" for feat in features if f\"{feat}_std\" in subset.columns]\n","        relevant_features = [f for f in relevant_features if f in subset.columns]\n","\n","        if len(relevant_features) == 0:\n","            continue\n","        subset_feat = subset[relevant_features].fillna(method='ffill').fillna(method='bfill')\n","\n","        iso_forest = IsolationForest(contamination=0.05, random_state=1)\n","        preds = iso_forest.fit_predict(subset_feat)\n","        anomaly_idx = subset.index[preds == -1]\n","\n","        for ts in anomaly_idx:\n","            anomaly_records.append({\n","                \"timestamp\": ts,\n","                \"cluster\": cluster_id,\n","                \"top_features\": \", \".join(relevant_features[:3])\n","            })\n","    anomaly_df = pd.DataFrame(anomaly_records)\n","    if not anomaly_df.empty:\n","        anomaly_df.to_csv(\"anomalies.csv\", index=False)\n","    return anomaly_df\n","\n","def forecast_temperature(df, features, shutdown_flag):\n","    target = \"Cyclone_Inlet_Gas_Temp\"\n","    horizon = 12\n","\n","    working_df = df.loc[~shutdown_flag].copy()\n","    ts = working_df[target].dropna()\n","\n","    split_index = int(len(ts) * 0.8)\n","    train_set, test_set = ts[:split_index], ts[split_index:]\n","\n","    persist_preds = []\n","    persist_actual = []\n","    for start in range(0, len(test_set) - horizon, horizon):\n","        last_val = train_set.iloc[-1] if start == 0 else test_set.iloc[start - 1]\n","        forecast_vals = [last_val] * horizon\n","        actual_vals = test_set.iloc[start: start + horizon].values\n","        persist_preds.extend(forecast_vals)\n","        persist_actual.extend(actual_vals)\n","\n","    persist_rmse = np.sqrt(mean_squared_error(persist_actual, persist_preds))\n","\n","    try:\n","        arima_model = ARIMA(train_set, order=(1,1,1))\n","        arima_fit = arima_model.fit()\n","        arima_preds = []\n","        arima_actual = []\n","        for start in range(0, len(test_set) - horizon, horizon):\n","            forecast_vals = arima_fit.forecast(steps=horizon)\n","            actual_vals = test_set.iloc[start: start + horizon].values\n","            arima_preds.extend(forecast_vals)\n","            arima_actual.extend(actual_vals)\n","        arima_rmse = np.sqrt(mean_squared_error(arima_actual, arima_preds))\n","    except Exception:\n","        arima_preds = persist_preds\n","        arima_rmse = persist_rmse\n","\n","    results_df = pd.DataFrame({\n","        \"actual\": persist_actual[:len(arima_preds)],\n","        \"persistence\": persist_preds[:len(arima_preds)],\n","        \"arima\": arima_preds[:len(arima_preds)]\n","    })\n","    results_df.to_csv(\"forecast_results.csv\", index=False)\n","\n","    plt.figure(figsize=(16, 8))\n","    plt.plot(results_df[\"actual\"][:200], label=\"Actual\")\n","    plt.plot(results_df[\"persistence\"][:200], label=\"Persistence\")\n","    plt.plot(results_df[\"arima\"][:200], label=\"ARIMA\")\n","    plt.title(\"Temperature Forecasting\")\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.savefig(\"plots/forecast_plot.png\", dpi=300)\n","    plt.close()\n","\n","    return results_df\n","\n","def main():\n","    try:\n","        df, sensors = load_data()\n","        exploratory_analysis(df, sensors)\n","        shutdown_flags, shutdown_df = identify_shutdowns(df, sensors)\n","        clustered_df, cluster_labels, cluster_summary = cluster_states(df, sensors, shutdown_flags)\n","        anomalies_df = detect_anomalies(clustered_df, cluster_labels, sensors)\n","        forecast_results = forecast_temperature(df, sensors, shutdown_flags)\n","        print(\"Process completed. CSV files and plots are saved in the current directory.\")\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"Xz_G0dTvCVQ4","outputId":"8dc7fbd8-3a4b-4f9a-b6d1-9ba749c5da99","executionInfo":{"status":"error","timestamp":1759569707032,"user_tz":-330,"elapsed":75,"user":{"displayName":"Ayesha Moghal","userId":"13341639430554687722"}}},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3387050089.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-transformers is required. Install with 'pip install sentence-transformers'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from sentence_transformers.backend import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mexport_dynamic_quantized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexport_optimized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_openvino_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_optimized_onnx_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_dynamic_quantized_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_static_quantized_openvino_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/backend/load.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_save_pretrained_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_should_export\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_warn_to_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .utils import (\n\u001b[1;32m     29\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .auto_docstring import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mClassAttrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mClassDocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/auto_docstring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0m_prepare_output_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# required for @can_return_tuple decorator to work with torchdynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_debugging_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_addition_debugger_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[0;31m# quantization depends on torch.fx and torch.ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m \u001b[0;31m# Import quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquantization\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;31m# Import the quasi random sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquant_type\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantization_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/quantization/qconfig.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m from torch.ao.quantization.qconfig import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_add_module_to_qconfig_obs_ctr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0m_assert_valid_qconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from .pt2e._numeric_debugger import (  # noqa: F401\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcompare_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mCUSTOM_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/_numeric_debugger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_sqnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt2e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbfs_trace_with_node_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/graph_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m from torch.fx.passes.utils.source_matcher_utils import (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPassResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPassManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/passes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgraph_drawer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_manipulation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet_min_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moperator_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/passes/graph_drawer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_format_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_qualified_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperator_schemas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_prop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mShapeProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mExecute\u001b[0m \u001b[0man\u001b[0m \u001b[0mFX\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNode\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"]}],"source":["#Task 2\n","import os\n","import re\n","import logging\n","from pathlib import Path\n","from typing import List, Dict, Tuple\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import numpy as np\n","import pandas as pd\n","\n","# All library imports are fail-fast now:\n","try:\n","    import faiss\n","except ImportError:\n","    raise ImportError(\"faiss-cpu is required. Install with 'pip install faiss-cpu'.\")\n","\n","try:\n","    from sentence_transformers import SentenceTransformer\n","except ImportError:\n","    raise ImportError(\"sentence-transformers is required. Install with 'pip install sentence-transformers'.\")\n","\n","try:\n","    from transformers import pipeline\n","except ImportError:\n","    raise ImportError(\"transformers is required. Install with 'pip install transformers'.\")\n","\n","try:\n","    import pdfplumber\n","    import PyPDF2\n","except ImportError:\n","    raise ImportError(\"pdfplumber and PyPDF2 required. Install with 'pip install pdfplumber PyPDF2'.\")\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(\"alt_rag\")\n","\n","class CycloneDocumentReader:\n","    def read_pdf(self, path: str) -> Tuple[str, Dict]:\n","        txt, meta = \"\", {\"path\": os.path.basename(path), \"pages\": 0}\n","        try:\n","            with pdfplumber.open(path) as doc:\n","                meta[\"pages\"] = len(doc.pages)\n","                for idx, pg in enumerate(doc.pages):\n","                    ptxt = pg.extract_text()\n","                    if ptxt:\n","                        txt += f\"\\n[Page {idx+1}]\\n{ptxt}\\n\"\n","        except Exception as e:\n","            logger.warning(f\"pdfplumber failed: {e}\")\n","            try:\n","                reader = PyPDF2.PdfReader(path)\n","                meta[\"pages\"] = len(reader.pages)\n","                for idx, pg in enumerate(reader.pages):\n","                    ptxt = pg.extract_text()\n","                    if ptxt:\n","                        txt += f\"\\n[Page {idx+1}]\\n{ptxt}\\n\"\n","            except Exception as e2:\n","                logger.error(f\"PyPDF2 failed: {e2}\")\n","        return self.clean_text(txt), meta\n","\n","    def clean_text(self, txt: str) -> str:\n","        txt = re.sub(r'\\s+', ' ', txt)\n","        txt = re.sub(r'\\[Page \\d+\\]', '', txt)\n","        txt = txt.replace('\\x00', '').replace('\\ufffd', '')\n","        return txt.strip()\n","\n","class TextChunker:\n","    def __init__(self, chunk_length: int = 400, overlap_size: int = 40):\n","        self.chunk_length = chunk_length\n","        self.overlap_size = overlap_size\n","\n","    def chunk_text(self, text: str, meta: Dict) -> List[Dict]:\n","        segments = []\n","        tokens = text.split('. ')\n","        buf = \"\"\n","        for idx, sent in enumerate(tokens):\n","            if len(buf) + len(sent) > self.chunk_length and buf:\n","                segments.append({\"text\": buf, \"metadata\": dict(meta, chunk_id=idx)})\n","                buf = buf[-self.overlap_size:] + \". \" + sent if self.overlap_size else sent\n","            else:\n","                buf += \". \" + sent if buf else sent\n","        if buf:\n","            segments.append({\"text\": buf, \"metadata\": dict(meta, chunk_id=len(tokens))})\n","        return segments\n","\n","class SemanticIndexer:\n","    def __init__(self, model_id: str = \"all-MiniLM-L6-v2\"):\n","        self.model = SentenceTransformer(model_id)\n","        self.dim = self.model.get_sentence_embedding_dimension()\n","        self.index = faiss.IndexFlatIP(self.dim)\n","        self.segments = []\n","\n","    def add_segments(self, records: List[Dict]):\n","        chunks = [rec[\"text\"] for rec in records]\n","        vectors = self.model.encode(chunks, show_progress_bar=True)\n","        faiss.normalize_L2(vectors)\n","        self.index.add(np.array(vectors).astype('float32'))\n","        for i, rec in enumerate(records):\n","            rec[\"embedding\"] = vectors[i]\n","        self.segments.extend(records)\n","\n","    def retrieve(self, query: str, k: int = 5) -> List[Tuple[Dict, float]]:\n","        query_vec = self.model.encode([query])[0].astype('float32').reshape(1, -1)\n","        faiss.normalize_L2(query_vec)\n","        D, I = self.index.search(query_vec, k)\n","        return [(self.segments[i], float(D[0][j])) for j, i in enumerate(I[0]) if i < len(self.segments)]\n","\n","class SimpleLLM:\n","    def __init__(self, model_name: str = \"gpt2\"):\n","        try:\n","            self.pipe = pipeline(\"text-generation\", model=model_name, tokenizer=model_name, max_length=1100)\n","        except Exception as e:\n","            logger.error(f\"Could not initialize LLM: {e}\")\n","            self.pipe = None\n","\n","    def generate(self, prompt: str) -> str:\n","        if not self.pipe:\n","            return \"LLM is unavailable.\"\n","        result = self.pipe(prompt, max_length=len(prompt)+320, pad_token_id=self.pipe.tokenizer.eos_token_id)\n","        output = result[0]['generated_text']\n","        idx = output.find(\"Answer:\") + 7 if \"Answer:\" in output else None\n","        return output[idx:].strip() if idx is not None else output\n","\n","class CycloneRAG:\n","    def __init__(self, chunk_length: int = 400, overlap_size: int = 40):\n","        self.reader = CycloneDocumentReader()\n","        self.chunker = TextChunker(chunk_length, overlap_size)\n","        self.indexer = SemanticIndexer()\n","        self.llm = SimpleLLM()\n","        self.conf_limit = 0.28\n","\n","    def ingest_folder(self, doc_folder: str):\n","        files = list(Path(doc_folder).glob(\"*\"))\n","        logger.info(f\"Ingesting {len(files)} files from {doc_folder}\")\n","        segments = []\n","        for f in files:\n","            if str(f).endswith(\".pdf\"):\n","                txt, meta = self.reader.read_pdf(str(f))\n","            else:\n","                txt = open(f, \"r\").read()\n","                meta = {\"path\": os.path.basename(str(f)), \"pages\": None}\n","            chunks = self.chunker.chunk_text(txt, meta)\n","            segments.extend(chunks)\n","        self.indexer.add_segments(segments)\n","\n","    def answer(self, q: str, top_k: int = 5) -> Dict:\n","        retrieved = self.indexer.retrieve(q, top_k)\n","        top_chunks = [rec for rec, sc in retrieved if sc >= self.conf_limit]\n","        prompt_ctx = \"\\n\".join([rec[\"text\"] for rec in top_chunks[:3]])\n","        prompt = f\"Based on cyclone documentation, answer and cite sources.\\nContext:\\n{prompt_ctx}\\nQuestion: {q}\\nAnswer:\"\n","        ans = self.llm.generate(prompt)\n","        sources = set(rec[\"metadata\"][\"path\"] for rec in top_chunks)\n","        return {\n","            \"query\": q,\n","            \"answer\": ans,\n","            \"confidence\": len(ans)/320 + 0.24,\n","            \"sources\": list(sources),\n","            \"chunk_count\": len(top_chunks)\n","        }\n","\n","def make_docs():\n","    os.makedirs(\"docs\", exist_ok=True)\n","    docs_data = [\n","        (\"docs/manual.txt\",\n","        \"Cyclone Operation Manual.\\nNormal inlet temp is 400-500C. Shutdown below 300C. Pressure should remain negative. Draft pressure above zero triggers shutdown.\"),\n","        (\"docs/maint.txt\",\n","        \"Maintenance Guide.\\nLog all pressure and temperature daily. Sudden drops may indicate fan or feed failure. Temperature spikes can result from overfeeding or cooling system issues.\")\n","    ]\n","    for name, text in docs_data:\n","        with open(name, \"w\") as f:\n","            f.write(text)\n","    print(\"Sample cyclone docs created in ./docs\")\n","\n","def main():\n","    print(\"\\n== Cyclone RAG Demo ==\")\n","    print(\"Author: AYESHA M\\n------------------------\")\n","    if not os.path.exists(\"docs\") or not os.listdir(\"docs\"):\n","        make_docs()\n","    pipeline = CycloneRAG(chunk_length=420, overlap_size=32)\n","    pipeline.ingest_folder(\"docs\")\n","\n","    queries = [\n","        \"What triggers a shutdown in cyclone separators?\",\n","        \"How do you interpret a temperature spike?\",\n","        \"Describe regular maintenance for cyclones.\",\n","        \"What is the safe range for inlet gas temperature?\",\n","        \"What causes sudden temperature drops?\"\n","    ]\n","    print(\"Testing RAG with sample questions:\")\n","    for q in queries:\n","        result = pipeline.answer(q)\n","        print(f\"\\nQuery: {q}\\nAnswer: {result['answer']}\\nConfidence: {result['confidence']:.2f}\\nSources: {result['sources']}\")\n","\n","    df = pd.DataFrame([pipeline.answer(q) for q in queries])\n","    df.to_csv(\"rag_results.csv\", index=False)\n","    print(\"Results saved to rag_results.csv\")\n","\n","    print(\"\\nInteractive mode. Type 'quit' to exit.\")\n","    while True:\n","        user_q = input(\"Ask your question: \").strip()\n","        if user_q.lower() in [\"quit\", \"exit\"]:\n","            break\n","        response = pipeline.answer(user_q)\n","        print(f\"\\nAnswer: {response['answer']}\\nSources: {response['sources']}\\n\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20ae7542","executionInfo":{"status":"ok","timestamp":1759569328326,"user_tz":-330,"elapsed":10919,"user":{"displayName":"Ayesha Moghal","userId":"13341639430554687722"}},"outputId":"608f38d3-51f4-41a0-f10c-90f0df19ab24"},"source":["# Install sentence-transformers and its dependencies\n","!pip install sentence-transformers -U"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Unf5nPEqNH_T"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8210,"status":"ok","timestamp":1759569221123,"user":{"displayName":"Ayesha Moghal","userId":"13341639430554687722"},"user_tz":-330},"id":"zyLkU776Cpdv","outputId":"6a4f14ca-81ee-4951-f9a6-073464026500"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n","Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.12.0\n"]}],"source":["pip install faiss-cpu"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13311,"status":"ok","timestamp":1759569234477,"user":{"displayName":"Ayesha Moghal","userId":"13341639430554687722"},"user_tz":-330},"id":"NegwuH6LC1vU","outputId":"d5ddd6d2-0eda-476b-8b42-d1b289fcc6a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pdfplumber\n","  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting pdfminer.six==20250506 (from pdfplumber)\n","  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n","Collecting pypdfium2>=4.18.0 (from pdfplumber)\n","  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n","Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdfium2, PyPDF2, pdfminer.six, pdfplumber\n","Successfully installed PyPDF2-3.0.1 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"]}],"source":["pip install pdfplumber PyPDF2"]},{"cell_type":"markdown","source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," Task 1: Machine Data Analysis – Summary Notes\n","1. Data Loading & Preprocessing\n","\n","Loaded 3 years of time-series data from data.xlsx, with ~370,000 rows at 5-minute intervals.\n","\n","Cleaned sensor data by replacing invalid entries (\"I/O Timeout\", \"No Data\", etc.) with NaN.\n","\n","Converted timestamp column to datetime and set it as index.\n","\n","Enforced strict 5-minute regularity in the time index for consistency.\n","\n","Used pandas, numpy for preprocessing.\n","\n"," 2. Exploratory Analysis\n","\n","Plotted correlation heatmap of all features to understand inter-variable relationships.\n","\n","Generated 2-week sample trend plots for key variables to observe normal behavior and trends.\n","\n","Outputs saved:\n","\n","plots/correlation_heatmap.png\n","\n","plots/sample_trends.png\n","\n"," 3. Shutdown / Idle Period Detection\n","\n","Defined shutdowns based on low quantile thresholds of 3 variables:\n","\n","Cyclone_Inlet_Gas_Temp\n","\n","Cyclone_Gas_Outlet_Temp\n","\n","Cyclone_Material_Temp\n","\n","Tagged shutdown periods using boolean masks and computed:\n","\n","Start, end, and duration of each shutdown.\n","\n","Total number of shutdown events and total downtime in hours.\n","\n","Visualized shutdowns over a sample period.\n","\n","Outputs saved:\n","\n","shutdown_periods.csv\n","\n","plots/shutdown_periods.png\n","\n"," 4. Operational State Segmentation (Clustering)\n","\n","Excluded shutdown data and used only active operation data.\n","\n","Created rolling features:\n","\n","Rolling mean, std, lag1, and diff for each sensor variable.\n","\n","Used KMeans (k=4) with StandardScaler to cluster data into interpretable machine states:\n","\n","Examples: Normal, High Load, Degraded, Transitional.\n","\n","Performed PCA-based visualization of cluster spread.\n","\n","Outputs saved:\n","\n","cluster_summary.csv\n","\n","plots/clusters_pca.png\n","\n"," 5. Contextual Anomaly Detection\n","\n","For each cluster:Trained Isolation Forest using relevant rolling features.\n","\n","Flagged anomalies as outliers in the context of their operational state.\n","\n","Generated a consolidated list of anomalies with timestamp, cluster ID, and top features.\n","\n","Outputs saved:\n","\n","anomalies.csv\n","\n"," 6. Short-Term Forecasting (1-Hour Horizon)\n","\n","Forecasted Cyclone_Inlet_Gas_Temp 12 steps (1 hour ahead).\n","\n","Compared:\n","\n","Persistence baseline (last observed value).\n","\n","ARIMA (1,1,1) model from statsmodels.\n","\n","Used 80-20 train-test split and computed RMSE for both models.\n","\n","Visualized actual vs predicted values for 200 steps.\n","\n","Outputs saved:\n","\n","forecast_results.csv\n","\n","plots/forecast_plot.png\n","\n"," 7. Key Libraries Used\n","pandas, numpy, matplotlib, seaborn\n","sklearn (KMeans, StandardScaler, IsolationForest)\n","statsmodels (ARIMA)\n","\n","📂 Output Files\n","Filename\tDescription\n","shutdown_periods.csv\tDetected shutdown events with start, end, duration\n","anomalies.csv\tAnomaly records with cluster context\n","cluster_summary.csv\tCluster-level summary statistics\n","forecast_results.csv\tForecast vs actual comparison\n","plots/ folder\tAll generated visualizations (PNG)\n","\n","\n","⚙️ How to Run\n","\n","Install dependencies:\n","\n","pip install -r requirements.txt\n","\n","Run the main analysis:\n","\n","python task1_analysis.py\n","\n","\n","OR use:\n","\n","jupyter notebook task1_analysis.ipynb\n","\n","\n","All outputs will be saved in the current directory or plots/.\n","\n","```\n","# This is formatted as code\n","```\n","\n"],"metadata":{"id":"t2TWL1koM1iL"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1vMQu_N1BsLEkuRaBuWiHGuLf-7q9XNcX","authorship_tag":"ABX9TyNSSn8I2iNevIg2847bzq7j"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}